{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9f8df1-19c6-4305-802b-fcd8b1c6519e",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1363f06-4b9c-47e3-8093-5bce617845b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721646-1025-4819-99dd-df5f7055ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Packages\n",
    "import copy;\n",
    "import cv2;\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from glob import iglob\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from operator import __add__\n",
    "import os\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import pandas as pd;\n",
    "from PIL import Image, ImageOps\n",
    "import PIL\n",
    "import pingouin as pg\n",
    "from random import randint\n",
    "import random;\n",
    "import sys\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.measurements import label\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from skimage import transform as tf\n",
    "from skimage import morphology\n",
    "from skimage.morphology import skeletonize\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules import Conv2d, Module\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2aaf2-2c28-447d-8502-4fd088b8ff22",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878e3f1-5f40-4879-8360-d8697bac0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=48, dilation=1,groups=1,bias=False, padding_mode=\"reflect\"): \n",
    "        super().__init__()\n",
    "        self.is_calculated = False\n",
    "\n",
    "        self.conv_layer = Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\n",
    "        self.kernel_size = self.conv_layer.kernel_size\n",
    "\n",
    "        # small addition to avoid division by zero\n",
    "        self.delta = 1e-3\n",
    "\n",
    "        #Frequency\n",
    "        self.freq = Parameter(\n",
    "            (math.pi / 2)\n",
    "            * math.sqrt(2)\n",
    "            ** (-torch.randint(0, 5, (out_channels, in_channels))).type(torch.Tensor),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        #Theta\n",
    "        self.theta = Parameter(\n",
    "            (math.pi / 8)\n",
    "            * torch.randint(0, 8, (out_channels, in_channels)).type(torch.Tensor),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        #Sigma\n",
    "        self.sigma = Parameter(math.pi / self.freq, requires_grad=True)\n",
    "\n",
    "        #Psi\n",
    "        self.psi = Parameter(\n",
    "            math.pi * torch.rand(out_channels, in_channels), requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.x0 = Parameter(\n",
    "            torch.ceil(torch.Tensor([self.kernel_size[0] / 2]))[0], requires_grad=False\n",
    "        )\n",
    "        self.y0 = Parameter(\n",
    "            torch.ceil(torch.Tensor([self.kernel_size[1] / 2]))[0], requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.y, self.x = torch.meshgrid(\n",
    "            [\n",
    "                torch.linspace(-self.x0 + 1, self.x0 + 0, self.kernel_size[0]),\n",
    "                torch.linspace(-self.y0 + 1, self.y0 + 0, self.kernel_size[1]),\n",
    "            ]\n",
    "        )\n",
    "        self.y = Parameter(self.y.clone())\n",
    "        self.x = Parameter(self.x.clone())\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.empty(self.conv_layer.weight.shape, requires_grad=True),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "        self.register_parameter(\"freq\", self.freq)\n",
    "        self.register_parameter(\"theta\", self.theta)\n",
    "        self.register_parameter(\"sigma\", self.sigma)\n",
    "        self.register_parameter(\"psi\", self.psi)\n",
    "        self.register_parameter(\"x_shape\", self.x0)\n",
    "        self.register_parameter(\"y_shape\", self.y0)\n",
    "        self.register_parameter(\"y_grid\", self.y)\n",
    "        self.register_parameter(\"x_grid\", self.x)\n",
    "        self.register_parameter(\"weight\", self.weight)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        if self.training:\n",
    "            self.calculate_weights()\n",
    "            self.is_calculated = False\n",
    "        if not self.training:\n",
    "            if not self.is_calculated:\n",
    "                self.calculate_weights()\n",
    "                self.is_calculated = True\n",
    "        return self.conv_layer(input_tensor)\n",
    "\n",
    "    def calculate_weights(self):\n",
    "        for i in range(self.conv_layer.out_channels):\n",
    "            for j in range(self.conv_layer.in_channels):\n",
    "                sigma = self.sigma[i, j].expand_as(self.y) \n",
    "                freq = self.freq[i, j].expand_as(self.y) \n",
    "                theta = self.theta[i, j].expand_as(self.y) \n",
    "                psi = self.psi[i, j].expand_as(self.y) \n",
    "\n",
    "                rotx = self.x * torch.cos(theta) + self.y * torch.sin(theta)\n",
    "                roty = -self.x * torch.sin(theta) + self.y * torch.cos(theta)\n",
    "\n",
    "                g = torch.exp(\n",
    "                    -0.5 * ((rotx ** 2 + roty ** 2) / (sigma + self.delta) ** 2)\n",
    "                )\n",
    "                g = g * torch.cos(freq * rotx + psi)\n",
    "                g = g / (2 * math.pi * sigma ** 2)\n",
    "                self.conv_layer.weight.data[i, j] = g\n",
    "\n",
    "    def _forward_unimplemented(self, *inputs: Any):\n",
    "        \"\"\"\n",
    "        code checkers makes implement this method,\n",
    "        looks like error in PyTorch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class filt_cat(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1, x2):     \n",
    "\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x;\n",
    "\n",
    "class SNP_Net(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SNP_Net, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.g0 = GaborConv2d(n_channels, n_channels, kernel_size=(96, 96))\n",
    "        self.fc = filt_cat(n_channels, 2*n_channels)\n",
    "        self.inc = DoubleConv(2*n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.g0(x)\n",
    "        x0 = self.fc(f,x);\n",
    "        x1 = self.inc(x0)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        m = nn.Softmax(dim=1)\n",
    "        logits = m(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549014b-f39d-489c-9898-4d90f6e3283e",
   "metadata": {},
   "source": [
    "Segmentation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa231cdc-cc28-4db6-8f2d-294057fe8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_metrics(confusion_matrix, per_class=False):\n",
    "    ret = {\n",
    "        'mIoU': confusion_matrix.get_mean_class_iou(),\n",
    "        'mDice': confusion_matrix.get_mean_class_dice(),\n",
    "        'oAcc': confusion_matrix.get_overall_accuracy(),\n",
    "        'mSens': confusion_matrix.get_mean_class_sensitivity(),\n",
    "        'mSpec': confusion_matrix.get_mean_class_specificity(),\n",
    "        'mF1': confusion_matrix.get_mean_class_f1(),\n",
    "    }\n",
    "    if per_class:\n",
    "        for c,v in enumerate(confusion_matrix.get_dice_per_class()):\n",
    "            ret['Dice{}'.format(c)] = v\n",
    "        for c,v in enumerate(confusion_matrix.get_sensitivity_per_class()):\n",
    "            ret['Sens{}'.format(c)] = v\n",
    "        for c,v in enumerate(confusion_matrix.get_specificity_per_class()):\n",
    "            ret['Spec{}'.format(c)] = v\n",
    "        for c,v in enumerate(confusion_matrix.get_f1_per_class()):\n",
    "            ret['F1{}'.format(c)] = v\n",
    "        for c,v in enumerate(confusion_matrix.get_ground_truth_ratios()):\n",
    "            ret['RGt{}'.format(c)] = v\n",
    "        for c,v in enumerate(confusion_matrix.get_prediction_ratios()):\n",
    "            ret['RPr{}'.format(c)] = v\n",
    "    return ret\n",
    "\n",
    "\n",
    "class ConfusionMatrix():\n",
    "    \"\"\"Maintains a running confusion matrix for a K-class classification problem.\n",
    "    Rows corresponds to ground-truth targets and columns corresponds to predicted targets.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.cm = np.zeros(shape=(k, k), dtype=object)  # array of python ints for unlimited precision\n",
    "        self.k = k\n",
    "\n",
    "    def reset(self):\n",
    "        self.cm.fill(0)\n",
    "\n",
    "    def add(self, predicted, target):\n",
    "        \"\"\"\n",
    "        Adds new results to the confusion matrix. Filters elements without ground truth (class = -100).\n",
    "\n",
    "        :param predicted: N, BHW or BDHW-sized tensor of class integers or NK, BKHW or BKDHW-sized tensor\n",
    "                          of predicted probabilities\n",
    "        :param target: N, BHW or BDHW-sized tensor of class integers or NK, BKHW or BKDHW-sized tensor\n",
    "                          of one-hot encoded classes\n",
    "        \"\"\"\n",
    "        # TODO: rewrite into pytorch for speed (no need to copy back to device)\n",
    "\n",
    "        if isinstance(predicted, torch.Tensor):\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "        if isinstance(target, torch.Tensor):\n",
    "            target = target.detach().cpu().numpy()\n",
    "\n",
    "        if np.ndim(predicted) > 1 and predicted.shape[1] == self.k:\n",
    "            predicted = np.argmax(predicted, 1)\n",
    "        else:\n",
    "            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n",
    "                'predicted values are not between 0 and k-1'\n",
    "\n",
    "        if np.ndim(target) > 1 and target.shape[1] == self.k:\n",
    "            assert (target >= 0).all() and (target <= 1).all(), \\\n",
    "                'in one-hot encoding, target values should be 0 or 1'\n",
    "            invalid_idx = target.sum(1) < 0.5\n",
    "            target = np.argmax(target, 1)\n",
    "            target[invalid_idx] = -100\n",
    "\n",
    "        predicted = np.ravel(predicted)\n",
    "        target = np.ravel(target)\n",
    "        assert predicted.shape[0] == target.shape[0], \\\n",
    "            'number of targets and predicted outputs do not match'\n",
    "\n",
    "        # Remove predictions for elements without ground truth\n",
    "        valid_idx = target != -100\n",
    "        target = target[valid_idx]\n",
    "        predicted = predicted[valid_idx]\n",
    "\n",
    "        # from https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n",
    "        # (sklearn.metrics.confusion_matrix is 100x slower)\n",
    "\n",
    "        \n",
    "        x = predicted + self.k * target\n",
    " \n",
    "        \n",
    "        bincount_2d = np.bincount(x.astype(np.int64),\n",
    "                                  minlength=self.k ** 2)\n",
    "        \n",
    "        \n",
    "        assert bincount_2d.size == self.k ** 2\n",
    "        cm = bincount_2d.reshape((self.k, self.k))\n",
    "\n",
    "        self.cm += cm\n",
    "\n",
    "    def value(self, normalized=False):\n",
    "        \"\"\"\n",
    "        :return confusion matrix of K rows and K columns\n",
    "        \"\"\"\n",
    "        conf = self.cm.astype(np.float64)\n",
    "        if normalized:\n",
    "            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n",
    "        else:\n",
    "            return conf\n",
    "\n",
    "    def get_iou_per_class(self):\n",
    "        \"\"\"\n",
    "        :return per-class intersection-over-union / Jaccard coefficient (NaN if class not present nor predicted)\n",
    "        \"\"\"\n",
    "        cm = self.value()\n",
    "        tp = np.diag(cm)\n",
    "        rowsum = cm.sum(axis=0)\n",
    "        colsum = cm.sum(axis=1)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            return tp / (rowsum + colsum - tp)\n",
    "\n",
    "    def get_mean_class_iou(self):\n",
    "        \"\"\"\n",
    "        :return mean per-class intersection-over-union / Jaccard coefficient (over classes present)\n",
    "        \"\"\"\n",
    "        iou = self.get_iou_per_class()\n",
    "        iou = iou[~np.isnan(iou)]\n",
    "        return np.mean(iou)\n",
    "\n",
    "    def get_overall_accuracy(self):\n",
    "        \"\"\"\n",
    "        :return overall sensitivity (class-unspecific)\n",
    "        \"\"\"\n",
    "        cm = self.value()\n",
    "        tp = np.diag(cm).sum()\n",
    "        oa = tp / max(1, np.sum(cm))\n",
    "        return oa\n",
    "\n",
    "    def get_sensitivity_per_class(self):\n",
    "        \"\"\"\n",
    "        :return per-class sensitivity (NaN if class not present)\n",
    "        \"\"\"\n",
    "        cm = self.value()\n",
    "        tp = np.diag(cm)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            return tp / np.sum(cm, axis=1)\n",
    "\n",
    "    def get_mean_class_sensitivity(self):\n",
    "        \"\"\"\n",
    "        :return mean per-class sensitivity (over classes present)\n",
    "        \"\"\"\n",
    "        sens = self.get_sensitivity_per_class()\n",
    "        sens = sens[~np.isnan(sens)]\n",
    "        return np.mean(sens)\n",
    "\n",
    "\n",
    "    def get_specificity_per_class(self):\n",
    "        \"\"\"\n",
    "        :return per-class specificity (NaN if class not present)\n",
    "        \"\"\"\n",
    "        cm = copy.deepcopy(self.value())\n",
    "        tp = np.diag(copy.deepcopy(cm))\n",
    "        tn = copy.deepcopy(tp);\n",
    "        for i in range(0,len(tn)):\n",
    "          tn[i] = tp.sum() - tp[i];\n",
    "          cm[i,i] = 0;\n",
    "\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            return tn /(tn +  np.sum(cm, axis=0))\n",
    "\n",
    "    def get_mean_class_specificity(self):\n",
    "        \"\"\"\n",
    "        :return mean per-class specificity (over classes present)\n",
    "        \"\"\"\n",
    "        spec = self.get_specificity_per_class()\n",
    "        spec = spec[~np.isnan(spec)]\n",
    "        return np.mean(spec)\n",
    "\n",
    "    def get_f1_per_class(self):\n",
    "        \"\"\"\n",
    "        :return per-class f1 (NaN if class not present)\n",
    "        \"\"\"\n",
    "        cm = self.value()\n",
    "        tp = np.diag(cm)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            return tp / (0.5*(np.sum(cm, axis=1) + np.sum(cm, axis=0)))\n",
    "\n",
    "\n",
    "    def get_mean_class_f1(self):\n",
    "        \"\"\"\n",
    "        :return mean per-class f1 (over classes present)\n",
    "        \"\"\"\n",
    "        f1 = self.get_f1_per_class()\n",
    "        f1 = f1[~np.isnan(f1)]\n",
    "        return np.mean(f1)\n",
    "\n",
    "\n",
    "\n",
    "    def get_dice_per_class(self):\n",
    "        \"\"\"\n",
    "        :return per-class Dice coefficient (NaN if class not present nor predicted)\n",
    "        \"\"\"\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            jacc = self.get_iou_per_class()\n",
    "            return 2 * jacc / (jacc + 1)\n",
    "\n",
    "    def get_mean_class_dice(self):\n",
    "        \"\"\"\n",
    "        :return mean per-class Dice coefficient (over classes present)\n",
    "        \"\"\"\n",
    "        dice = self.get_dice_per_class()\n",
    "        dice = dice[~np.isnan(dice)]\n",
    "        return np.mean(dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ca019-325f-4cec-a7ef-f2f4294bac08",
   "metadata": {},
   "source": [
    "Weighted Average and StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39294e-9d71-4234-a995-08534fbeaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_and_std(values, weights):\n",
    "\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    \n",
    "    return (average, math.sqrt(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40f06c-b817-45a1-a8fc-a56953ceaa78",
   "metadata": {},
   "source": [
    "Gaussian Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942225f-88bc-4ea9-b2f8-b91fa152ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fspecial_gauss(size, sigma):\n",
    "\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2335f-6fa3-4105-be7f-7f671f797ad2",
   "metadata": {},
   "source": [
    "Find Branchpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec006a-bb62-40b5-880b-defc7e695153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_branchpoints(skel):\n",
    "    # Make our input nice, possibly necessary.\n",
    "    skel = skel.copy()\n",
    "    skel[skel!=0] = 1\n",
    "    skel = np.uint8(skel)\n",
    "\n",
    "    # Apply the convolution.\n",
    "    kernel = np.uint8([[1,  1, 1],\n",
    "                       [1, 10, 1],\n",
    "                       [1,  1, 1]])\n",
    "    src_depth = -1\n",
    "    filtered = cv2.filter2D(skel,src_depth,kernel)\n",
    "\n",
    "\n",
    "    out = np.zeros_like(skel)\n",
    "    out[np.where(filtered>12)] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82623c-bde6-4406-99e4-8c34b33424ba",
   "metadata": {},
   "source": [
    "Find Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684c383-c0fe-4efc-9d5a-847d3c739c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_endpoints(skel):\n",
    "    # Make our input nice, possibly necessary.\n",
    "    skel = skel.copy()\n",
    "    skel[skel!=0] = 1\n",
    "    skel = np.uint8(skel)\n",
    "    \n",
    "    skel = np.pad(skel, (1, 1), 'constant', constant_values=(0, 0))\n",
    "\n",
    "    # Apply the convolution.\n",
    "    kernel = np.uint8([[1,  1, 1],\n",
    "                       [1, 10, 1],\n",
    "                       [1,  1, 1]])\n",
    "    src_depth = -1\n",
    "    filtered = cv2.filter2D(skel,src_depth,kernel)\n",
    "\n",
    "    # Look through to find the value of 11.\n",
    "    # This returns a mask of the endpoints, but if you\n",
    "    # just want the coordinates, you could simply\n",
    "    # return np.where(filtered==11)\n",
    "    out = np.zeros_like(skel)\n",
    "    out[np.where(filtered==11)] = 1\n",
    "    out = out[1:-1,1:-1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a177d0-0544-44c9-ae96-12766589a370",
   "metadata": {},
   "source": [
    "Count Immune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703b701-db4d-457c-b80d-1c8f7af96c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_immune_cells(mask, mask_AAR):\n",
    "\n",
    "    #Binary Mask of Immune Cells\n",
    "    binary = np.zeros(np.shape(mask)) \n",
    "    binary[np.where(mask==2)] = 1;\n",
    "\n",
    "\n",
    "    #Create Unique labels\n",
    "    islands = np.zeros(np.shape(binary))              \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    labeled, ncomponents = label(binary, structure)\n",
    "    labeled = labeled*binary;\n",
    "\n",
    "    #Count Immune Cells\n",
    "    count = 0;\n",
    "    unique = np.unique(labeled)\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            if(np.shape(np.where(labeled ==unique_label))[1]>9):\n",
    "                count = count + 1;    \n",
    "    \n",
    "\n",
    "    #Normalize by Area of SNP\n",
    "    count = count * np.size(mask_AAR)/np.shape(np.where(mask_AAR==1))[1] \n",
    "  \n",
    "    return count;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a25b0d-6526-49fc-b7b8-d0c47ba9722b",
   "metadata": {},
   "source": [
    "Count Neuromas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d602c-26d2-4998-a1d8-09d8a6cd99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_neuromas(mask, mask_AAR):\n",
    "\n",
    "    #Binary Mask of Neuromas\n",
    "    binary = np.zeros(np.shape(mask)) \n",
    "    binary[np.where(mask==3)] = 1;\n",
    "\n",
    "\n",
    "    #Create Unique labels\n",
    "    islands = np.zeros(np.shape(binary))              \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    labeled, ncomponents = label(binary, structure)\n",
    "    labeled = labeled*binary;\n",
    "\n",
    "    #Count Neuromas\n",
    "    count = 0;\n",
    "    unique = np.unique(labeled)\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            if(np.shape(np.where(labeled ==unique_label))[1]>9):\n",
    "                count = count + 1;    \n",
    "    \n",
    "\n",
    "    #Normalize by Area of SNP\n",
    "    count = count * np.size(mask_AAR)/np.shape(np.where(mask_AAR==1))[1];\n",
    "  \n",
    "    return count;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5dad6-ba88-445e-bac9-4dcb4a1bb330",
   "metadata": {},
   "source": [
    "Count Junctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972eb8d-f10e-4e39-be34-c2437360384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_junctions(mask, mask_AAR):\n",
    "    \n",
    "  \n",
    "    #Remove Holes\n",
    "    skel = skeletonize(mask==1);\n",
    "    invert_skel = skel==0;\n",
    "    invert_skel = np.array(morphology.remove_small_objects(invert_skel, 10)) \n",
    "    skel = np.abs(invert_skel-1)\n",
    "    \n",
    "    #Skeletonize\n",
    "    skel = skeletonize(skel)\n",
    "\n",
    "    #Remove Boundarys\n",
    "    skel[0,:] = 0;\n",
    "    skel[:,0] = 0;\n",
    "    skel[-1,:] = 0;\n",
    "    skel[:,-1] = 0;\n",
    "\n",
    "    \n",
    "    #Find Branch Points\n",
    "    branchpoints = skeleton_branchpoints(skel);\n",
    "    bp = np.array(np.where(branchpoints==1))\n",
    "    \n",
    "    #Iterate through Branch Points, Remove Unncessary Pixels \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    for j in range(0, np.shape(bp)[1],1):\n",
    "\n",
    "        #Box Around Branch Point\n",
    "        box = skel[int(bp[0,j])-1:int(bp[0,j])+2, int(bp[1,j])-1:int(bp[1,j])+2].copy()\n",
    "        \n",
    "        #Before Removing Center\n",
    "        labeled, ncomponents = label(box, structure)\n",
    "        \n",
    "        #After Removing Center\n",
    "        box[1,1] = 0;     \n",
    "        labeled, ncomponents_new = label(box, structure)   \n",
    "        \n",
    "        #If Unchanged, Remove Center\n",
    "        if(ncomponents ==ncomponents_new ):\n",
    "            skel[int(bp[0,j]), int(bp[1,j])] = 0;\n",
    "\n",
    "\n",
    "    #Find Endpoints        \n",
    "    endpoints = skeleton_endpoints(skel);\n",
    "    ep = np.array(np.where(endpoints==1));        \n",
    "            \n",
    "    #Separate Branches\n",
    "    branchpoints = skeleton_branchpoints(skel);\n",
    "    skel_sep = skel.copy();\n",
    "    skel_sep[np.where(branchpoints==1)] = 0;     \n",
    "    \n",
    "    \n",
    "    #Create Unique labels for Branches\n",
    "    labeled, ncomponents = label(skel_sep, structure)\n",
    "    labeled = labeled*skel_sep;\n",
    "    unique = np.unique(labeled)\n",
    "    \n",
    "    \n",
    "    #Iterate through Branches\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            \n",
    "            #Current Branch\n",
    "            current = np.zeros(np.shape(labeled));\n",
    "            current[np.where(labeled==unique_label)] = 1;\n",
    "            \n",
    "            \n",
    "            #If Current Branch Contains Endpoint\n",
    "            summation= np.sum(np.multiply(current,endpoints))       \n",
    "            if(np.shape(np.where(labeled==unique_label))[1]<10 and summation>0):\n",
    "                \n",
    "                #Remove Small Branches\n",
    "                skel[np.where(labeled==unique_label)] = 0;\n",
    "\n",
    "\n",
    "    #Remove  Spurs \n",
    "    branchpoints = skeleton_branchpoints(skel);  \n",
    "    skel[np.where(branchpoints ==1)] = 0;    \n",
    "    struct1 = ndimage.generate_binary_structure(2, 2)\n",
    "    skel = ndimage.binary_dilation(skel , structure=struct1)\n",
    "    skel = skeletonize(skel)\n",
    "\n",
    "\n",
    "    #Count Branchpoints\n",
    "    branchpoints = skeleton_branchpoints(skel); \n",
    "    labeled, ncomponents = label(branchpoints, structure)\n",
    "    labeled = labeled*branchpoints;\n",
    "    count = np.shape(np.unique(labeled))[0];\n",
    "\n",
    "    #Normalize by Area of SNP\n",
    "    count = count * np.size(mask_AAR)/np.shape(np.where(mask_AAR==1))[1];\n",
    "       \n",
    "\n",
    "    return count;\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc463523-dd57-4fb4-9a07-c125046d7791",
   "metadata": {},
   "source": [
    "Nerve Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c7290-7d40-4bc6-8427-e7640ca12aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nerve_density(mask, mask_AAR):\n",
    "\n",
    "    #Find Nerve\n",
    "    nerve = np.shape(np.where(mask==1))[1]\n",
    "\n",
    "    \n",
    "    #Nerve Density\n",
    "    total = np.size(mask)\n",
    "    density = round(400*400*(nerve/total))\n",
    "\n",
    "    #Normalize by Area of SNP\n",
    "    density = density * np.size(mask_AAR)/np.shape(np.where(mask_AAR==1))[1];\n",
    "\n",
    "    return density;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5fa1b-21d0-42aa-b5e0-5b6a19a18973",
   "metadata": {},
   "source": [
    "Nerve Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8677b48-ce49-45b2-ab72-6bc3e995e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nerve_thickness(mask):\n",
    " \n",
    "    #Skeletonized Nerve\n",
    "    skel = skeletonize(mask==1);\n",
    "    skel = np.shape(np.where(skel==1))[1];\n",
    "\n",
    "    #Find Nerve \n",
    "    nerve = np.shape(np.where(mask==1))[1]\n",
    "\n",
    "\n",
    "    #Nerve Thickness\n",
    "    length = np.sqrt(np.size(mask));\n",
    "    thickness = (nerve/skel) *(400/length);\n",
    "\n",
    "    return thickness;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ded936-64f3-46df-9c60-e8cd906cc852",
   "metadata": {},
   "source": [
    "Nerve Tortuosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209d93a-1f57-4aed-a76a-61d43e825183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nerve_tortuosity(mask):\n",
    "    \n",
    "    \n",
    "    skel = skeletonize(mask==1);\n",
    "    branchpoints = skeleton_branchpoints(skel);\n",
    "    skel[np.where(branchpoints==1)] = 0;\n",
    "\n",
    "    #Initialize RMS Sum\n",
    "    tau_C_Sum = 0;\n",
    "    tau_L_Sum = 0;\n",
    "    tau_CL_Sum = 0;\n",
    "\n",
    "    #Iterate through objects of length greater than 1\n",
    "    islands = np.zeros(np.shape(skel))              \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    labeled, ncomponents = label(skel, structure)\n",
    "    labeled = labeled*skel;\n",
    "    unique = np.unique(labeled)\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            if(np.shape(np.where(labeled==unique_label))[1]>3):\n",
    "\n",
    "                #Find Endpoints\n",
    "                current_branch = np.zeros(np.shape(skel));\n",
    "                current_branch[np.where(labeled==unique_label)] = 1;\n",
    "                endpoints = skeleton_endpoints(current_branch);\n",
    "                ep = np.where(endpoints==1);\n",
    "                y_pos = ep[0];\n",
    "                x_pos = ep[1]\n",
    "\n",
    "                #Skip Issues\n",
    "                if(np.shape(np.where(endpoints==1))[1]==0):\n",
    "                    continue;\n",
    "                \n",
    "\n",
    "                #Find Straight-Line Distance\n",
    "                Lx = np.sqrt(np.power(y_pos[0]-y_pos[1],2)+np.power(x_pos[0]-x_pos[1],2));\n",
    "\n",
    "                #Pixels in Branch\n",
    "                locs = np.where(current_branch==1);\n",
    "                y_locs = locs[0];\n",
    "                x_locs = locs[1];\n",
    "\n",
    "                #Order Pixels in branch\n",
    "                distances = np.sqrt(np.power(y_locs-y_pos[0],2) + np.power(x_locs-x_pos[0],2));\n",
    "                idx = np.argsort(distances);\n",
    "                y_locs = y_locs[idx];\n",
    "                x_locs = x_locs[idx];\n",
    "\n",
    "\n",
    "\n",
    "                Lc=0;\n",
    "                tau_C=0;\n",
    "                for a in range(1,np.shape(y_locs)[0]):\n",
    "                    #Calculate First Differential\n",
    "                    first_x = x_locs[a]-x_locs[a-1];\n",
    "                    first_y = y_locs[a]-y_locs[a-1];\n",
    "\n",
    "\n",
    "                    if a>1:\n",
    "                      \n",
    "                        #Calculate Second Differential\n",
    "                        prev_x = x_locs[a-1]-x_locs[a-2];\n",
    "                        prev_y = y_locs[a-1]-y_locs[a-2];\n",
    "                        second_x = first_x-prev_x;\n",
    "                        second_y = first_y-prev_y;\n",
    "                        Ki = ((first_x*second_y)-(second_x*first_y))/np.power(np.power(first_x,2)+np.power(first_y,2),(3/2));\n",
    "                    else:\n",
    "                        Ki=0\n",
    "\n",
    "                    #Calculate Various Measures\n",
    "                    Lc = Lc + np.sqrt(np.power(first_x,2)+np.power(first_y,2));\n",
    "                    tau_C = tau_C + abs(Ki);\n",
    "\n",
    "                #Weight Different measures\n",
    "                tau_L = Lc/Lx;\n",
    "                tau_CL = tau_C/Lc;\n",
    "                tau_C_Sum = tau_C_Sum + tau_C*np.shape(np.where(labeled==unique_label))[1];\n",
    "                tau_L_Sum = tau_L_Sum + tau_L*np.shape(np.where(labeled==unique_label))[1];\n",
    "                tau_CL_Sum = tau_CL_Sum + tau_CL*np.shape(np.where(labeled==unique_label))[1];\n",
    "\n",
    "\n",
    "\n",
    "    #Normalize by Skel\n",
    "    skel_count = np.shape(np.where(skel==1))\n",
    "    skel_count = skel_count[1];  \n",
    "\n",
    "\n",
    "    #Add to Tortuosity List\n",
    "    return tau_C_Sum/skel_count;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbf4d0-2323-4391-9d90-efac07f6f7cd",
   "metadata": {},
   "source": [
    "Fix Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c1087-46fc-43b6-ba83-8cfc4db787f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mixture_1(prediction, class_1, class_2):\n",
    "\n",
    "    \n",
    "    # Class Locations\n",
    "    binary = np.zeros(np.shape(prediction)) \n",
    "    binary[np.where(prediction==class_1)] = 1;\n",
    "    binary[np.where(prediction==class_2)] = 1;\n",
    "    \n",
    "    #Create Labeled Islands\n",
    "    islands = np.zeros(np.shape(prediction))              \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    labeled, ncomponents = label(binary, structure)\n",
    "    labeled = labeled*binary;\n",
    "    \n",
    "    #Iterate through Islands\n",
    "    unique = np.unique(labeled)\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            \n",
    "            #If more than one class in island\n",
    "            if(np.unique(prediction[np.where(labeled==unique_label)]).size >1):\n",
    "                \n",
    "                                              \n",
    "                #More abundant class dominates\n",
    "                count_1 = np.shape(np.where(prediction[np.where(labeled==unique_label)]==class_1))[1];\n",
    "                count_2 = np.shape(np.where(prediction[np.where(labeled==unique_label)]==class_2))[1];\n",
    "                if( count_1>count_2):\n",
    "                    prediction[np.where(labeled==unique_label)] = class_1\n",
    "                else:\n",
    "                    prediction[np.where(labeled==unique_label)] = class_2\n",
    "                    \n",
    "    return prediction;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e933d-263f-4198-8625-0d9caa0b0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mixture_2(prediction, class_1, class_2):\n",
    "\n",
    "    \n",
    "    # Class Locations\n",
    "    binary = np.zeros(np.shape(prediction)) \n",
    "    binary[np.where(prediction==class_1)] = 1;\n",
    "    binary[np.where(prediction==class_2)] = 1;\n",
    "    \n",
    "    #Create Labeled Islands\n",
    "    islands = np.zeros(np.shape(prediction))              \n",
    "    structure = np.ones((3, 3), dtype=int) \n",
    "    labeled, ncomponents = label(binary, structure)\n",
    "    labeled = labeled*binary;\n",
    "    \n",
    "    #Iterate through Islands\n",
    "    unique = np.unique(labeled)\n",
    "    for unique_label in unique:\n",
    "        if(unique_label!=0):\n",
    "            \n",
    "            #If more than one class in island\n",
    "            if(np.unique(prediction[np.where(labeled==unique_label)]).size >1):\n",
    "                \n",
    "                                              \n",
    "                #More abundant class dominates\n",
    "                count_1 = np.shape(np.where(prediction[np.where(labeled==unique_label)]==class_1))[1];\n",
    "                count_2 = np.shape(np.where(prediction[np.where(labeled==unique_label)]==class_2))[1];\n",
    "                if( count_1<count_2):\n",
    "                    prediction[np.where(labeled==unique_label)] = class_2\n",
    "                    \n",
    "    return prediction;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecb7f2-fa4c-4539-94a1-389aa8c886e6",
   "metadata": {},
   "source": [
    "Fix Class within Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862c602-8b40-4485-8d36-04c410c80132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_inner(prediction):\n",
    "\n",
    "    #Iterate through Classes\n",
    "    for class_outer in range(1,4):\n",
    "\n",
    "        # Class Locations\n",
    "        binary = np.zeros(np.shape(prediction)) \n",
    "        binary[np.where(prediction==class_outer)] = 1;\n",
    "        \n",
    "        \n",
    "        #Fill Holes\n",
    "        binary = ndimage.binary_fill_holes(binary).astype(int)\n",
    "        binary[np.where(prediction==class_outer)] = 0;\n",
    "        \n",
    "        #Create Labeled Islands\n",
    "        structure = np.ones((3, 3), dtype=int) \n",
    "        labeled, ncomponents = label(binary, structure)\n",
    "        labeled = labeled*binary;\n",
    "        \n",
    "        \n",
    "        #Iterate through Islands\n",
    "        unique = np.unique(labeled)\n",
    "        for unique_label in unique:\n",
    "            if(unique_label!=0):    \n",
    "                \n",
    "                #If it doesn't contain background\n",
    "                if(0 not in np.unique(prediction[np.where(labeled==unique_label)])):\n",
    "                    prediction[np.where(labeled==unique_label)] = class_outer;\n",
    "    \n",
    "    return prediction;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ec37a-6362-4b80-9d49-0188e6e64e83",
   "metadata": {},
   "source": [
    "Remove Small Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de173642-a1cd-466a-9f8d-ad0c06c89f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small(prediction):\n",
    "\n",
    "    #Iterate through Classes\n",
    "    for current_class in range(1,4):\n",
    "\n",
    "        # Class Locations\n",
    "        binary = np.zeros(np.shape(prediction)) \n",
    "        binary[np.where(prediction==current_class)] = 1;\n",
    "        \n",
    "        \n",
    "        #Create Labeled Islands\n",
    "        islands = np.zeros(np.shape(prediction))              \n",
    "        structure = np.ones((3, 3), dtype=int) \n",
    "        labeled, ncomponents = label(binary, structure)\n",
    "        labeled = labeled*binary;\n",
    "\n",
    "        #Iterate through Islands\n",
    "        unique = np.unique(labeled)\n",
    "        for unique_label in unique:\n",
    "            if(unique_label!=0): \n",
    "                \n",
    "                if(np.shape(np.where(labeled ==unique_label))[1]<8):\n",
    "                    prediction[np.where(labeled==unique_label)] = 0;\n",
    "    \n",
    "    return prediction;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bba1c2-6c88-4b39-807d-004f5a79c630",
   "metadata": {},
   "source": [
    "Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add01db-53f1-4c1b-ad3c-cf3333e3ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_net(python_path, torch_path, img_path, Experiment_Name, Model_Name, tissues, channels, Plot_Figures, Save_Figures):\n",
    "    \n",
    " \n",
    "\n",
    "    #Cast to Cuda\n",
    "    CUDA_VISIBLE_DEVICES=2\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    if cuda:\n",
    "        FloatTensor = torch.cuda.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor\n",
    "    else:\n",
    "        FloatTensor = torch.FloatTensor\n",
    "        LongTensor = torch.LongTensor\n",
    "\n",
    "    #Attempt to use GPU instead of CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
    "\n",
    "\n",
    "    #Load Numpy Files\n",
    "    PID =  np.load(python_path + 'SNP_Net_Groups.npy');  \n",
    "    SNP = np.load(python_path + 'SNP_Net_Images.npy');\n",
    "    masks = np.load(python_path + 'SNP_Net_Masks.npy');\n",
    "    masks_AAR = np.load(python_path  + 'AAR_Net_Masks.npy');  \n",
    "\n",
    "\n",
    "    #Correct Applanation Artifact Removal\n",
    "    masks_AAR = (masks_AAR==0);\n",
    "    masks_AAR = masks_AAR *1.0;\n",
    "    \n",
    "    #Weight\n",
    "    weight_kernel = fspecial_gauss(96, 5);\n",
    "    \n",
    "        \n",
    "    #Initialize\n",
    "    fscores = np.zeros((np.shape(SNP)[0], tissues));\n",
    "    sensitivity = np.zeros((np.shape(SNP)[0], tissues));\n",
    "    specificity = np.zeros((np.shape(SNP)[0], tissues));\n",
    "    supports = np.zeros((np.shape(SNP)[0], tissues));\n",
    "    immune_counts = np.zeros((np.shape(SNP)[0], 2));\n",
    "    neuroma_counts = np.zeros((np.shape(SNP)[0], 2));\n",
    "    junction_counts = np.zeros((np.shape(SNP)[0], 2));\n",
    "    nerve_densities = np.zeros((np.shape(SNP)[0], 2));\n",
    "    nerve_thicknesses = np.zeros((np.shape(SNP)[0], 2));  \n",
    "    nerve_tortuosities = np.zeros((np.shape(SNP)[0], 2));   \n",
    "        \n",
    "    for t in range(0,21):    \n",
    "   \n",
    "\n",
    "        #Load Networks\n",
    "        validation_0 = np.load(torch_path  +'Validation_Dice_'+  Model_Name +'_' + Experiment_Name + '_Group_' + str(t)  +'_0.npy').squeeze();    \n",
    "        epoch_0 = np.argmax(validation_0[0,60:]) + 60;\n",
    "        net_0 = SNP_Net(n_channels=channels, n_classes=tissues);\n",
    "        net_0.load_state_dict(torch.load(torch_path + Model_Name + '_' + Experiment_Name + '_Group_' + str(t) +'_0_Epoch_' +str(epoch_0+1) +'.pth', map_location=device));\n",
    "        net_0.to(device=device);  \n",
    "        net_0.eval();                        \n",
    "        validation_1 = np.load(torch_path  +'Validation_Dice_'+  Model_Name +'_' + Experiment_Name + '_Group_' + str(t)  +'_1.npy').squeeze();    \n",
    "        epoch_1 =np.argmax(validation_1[0,60:]) + 60;                    \n",
    "        net_1 = SNP_Net(n_channels=channels, n_classes=tissues);\n",
    "        net_1.load_state_dict(torch.load(torch_path + Model_Name + '_' + Experiment_Name + '_Group_' + str(t) +'_1_Epoch_' +str(epoch_1+1) +'.pth', map_location=device));\n",
    "        net_1.to(device=device);  \n",
    "        net_1.eval();                                 \n",
    "        validation_2 = np.load(torch_path  +'Validation_Dice_'+  Model_Name +'_' + Experiment_Name + '_Group_' + str(t)  +'_2.npy').squeeze();    \n",
    "        epoch_2 =np.argmax(validation_2[0,60:]) + 60;                                   \n",
    "        net_2 = SNP_Net(n_channels=channels, n_classes=tissues);\n",
    "        net_2.load_state_dict(torch.load(torch_path + Model_Name + '_' + Experiment_Name + '_Group_' + str(t) +'_2_Epoch_' +str(epoch_2+1) +'.pth', map_location=device));\n",
    "        net_2.to(device=device);  \n",
    "        net_2.eval();                               \n",
    "        validation_3 = np.load(torch_path  +'Validation_Dice_'+  Model_Name +'_' + Experiment_Name + '_Group_' + str(t)  +'_3.npy').squeeze();    \n",
    "        epoch_3 =np.argmax(validation_3[0,60:]) + 60;                                 \n",
    "        net_3 = SNP_Net(n_channels=channels, n_classes=tissues);\n",
    "        net_3.load_state_dict(torch.load(torch_path + Model_Name + '_' + Experiment_Name + '_Group_' + str(t) +'_3_Epoch_' +str(epoch_3+1) +'.pth', map_location=device));\n",
    "        net_3.to(device=device);  \n",
    "        net_3.eval();                                 \n",
    "        validation_4 = np.load(torch_path  +'Validation_Dice_'+  Model_Name +'_' + Experiment_Name + '_Group_' + str(t)  +'_4.npy').squeeze();    \n",
    "        epoch_4 =np.argmax(validation_4[0,60:]) + 60;  \n",
    "        net_4 = SNP_Net(n_channels=channels, n_classes=tissues);\n",
    "        net_4.load_state_dict(torch.load(torch_path + Model_Name + '_' + Experiment_Name + '_Group_' + str(t) +'_4_Epoch_' +str(epoch_4+1) +'.pth', map_location=device));\n",
    "        net_4.to(device=device);  \n",
    "        net_4.eval();\n",
    "        \n",
    "        #Iterate through Images\n",
    "        for idx in range(0, np.shape(SNP)[0]):\n",
    "\n",
    "            if(PID[idx] !=t):  \n",
    "                continue;                \n",
    "            \n",
    "            print(idx)\n",
    "            \n",
    "            #Select Image\n",
    "            img = SNP[idx, :,:].copy();\n",
    "            mask = masks[idx, :,:].copy();\n",
    "            mask_AAR = masks_AAR[idx, :,:].copy();\n",
    "\n",
    "            #Normalize\n",
    "            img = img/np.max(img)\n",
    "            img = img - 0.5;    \n",
    "\n",
    "            #Initialize\n",
    "            collective = np.zeros((tissues,384,384));\n",
    "            weight = np.zeros((tissues,384,384));\n",
    "\n",
    " \n",
    "            #Iterate through Patches\n",
    "            for row in range(0,324,36):\n",
    "                for col in range(0,324,36):\n",
    "\n",
    "                    for flip_idx in range(0,4):\n",
    "\n",
    "                        #Select Window\n",
    "                        cropped_img = img[row:row+96,col:col+96].copy();\n",
    "\n",
    "\n",
    "\n",
    "                         #Flip Image\n",
    "                        if(flip_idx >1):\n",
    "                            cropped_img = np.fliplr(cropped_img).copy();\n",
    "                        if(flip_idx ==1 or flip_idx ==3):\n",
    "                            cropped_img = np.flipud(cropped_img).copy();               \n",
    "                        cropped_img = np.expand_dims(cropped_img,axis = 0);\n",
    "\n",
    "                        #Cast to Torch\n",
    "                        input_img = torch.from_numpy(cropped_img).unsqueeze(0).to(device=device, dtype=torch.float32);\n",
    "\n",
    "\n",
    "                        with torch.no_grad():\n",
    "\n",
    "                            #Predict the Mask                                         \n",
    "                            output = net_0(input_img) + net_1(input_img) + net_2(input_img) + net_3(input_img) + net_4(input_img);\n",
    "\n",
    "\n",
    "                            output = output.cpu().detach().numpy().squeeze();\n",
    "\n",
    "                            #Multiply with Gaussian Weight\n",
    "                            mult_output = np.multiply(softmax(output,axis = 0),weight_kernel);   \n",
    "\n",
    "                            #Flip\n",
    "                            flip_output = mult_output.copy()\n",
    "                            if(flip_idx >1):\n",
    "                                mult_output[0] = np.fliplr(mult_output[0]).copy();\n",
    "                                mult_output[1] = np.fliplr(mult_output[1]).copy();\n",
    "                                mult_output[2] = np.fliplr(mult_output[2]).copy();\n",
    "                                mult_output[3] = np.fliplr(mult_output[3]).copy();\n",
    "                            if(flip_idx ==1 or flip_idx ==3):\n",
    "                                mult_output[0] = np.flipud(mult_output[0]).copy();\n",
    "                                mult_output[1] = np.flipud(mult_output[1]).copy();\n",
    "                                mult_output[2] = np.flipud(mult_output[2]).copy();\n",
    "                                mult_output[3] = np.flipud(mult_output[3]).copy();\n",
    "\n",
    "                            #Add to Collective\n",
    "                            collective[:,row:row+96,col:col+96] += mult_output\n",
    "                            weight[:,row:row+96,col:col+96] +=weight_kernel ;\n",
    "\n",
    "\n",
    "            #Divide by Count\n",
    "            collective = np.divide(collective, weight);\n",
    "            prediction = np.argmax(collective, axis = 0);\n",
    "\n",
    "\n",
    "            #Applanatin Artifact Removal\n",
    "            prediction = prediction*mask_AAR;\n",
    "\n",
    "\n",
    "            #Fix Mixture\n",
    "            prediction = fix_mixture_1(prediction, 2, 3);\n",
    "            prediction = fix_mixture_2(prediction, 1, 2);\n",
    "\n",
    "            #Fix Classes within Class\n",
    "            prediction = fix_inner(prediction);\n",
    "\n",
    "\n",
    "            #Remove Small Objects\n",
    "            prediction = remove_small(prediction);\n",
    "\n",
    "\n",
    "\n",
    "            #Calculate Metrics\n",
    "            cm = ConfusionMatrix(tissues)\n",
    "            cm.add(prediction, mask)\n",
    "            acc_out = segmentation_metrics(cm);\n",
    "            sens_per_class = cm.get_sensitivity_per_class();\n",
    "            spec_per_class = cm.get_specificity_per_class();\n",
    "            sensitivity[idx,0:4] = sens_per_class;\n",
    "            specificity[idx,0:4] = spec_per_class;       \n",
    "            temp_1 =   mask.flatten();\n",
    "            temp_2 =   prediction.flatten();\n",
    "            x = np.array([0,1,2,3]).astype('float64');\n",
    "            temp_1 =  np.concatenate((temp_1, x))\n",
    "            temp_2 =  np.concatenate((temp_2, x))\n",
    "            precision,recall,fscore,support=score(temp_1,temp_2)\n",
    "            fscores[idx,0:4] = fscore;\n",
    "            supports[idx,0:4] = support -1;\n",
    "\n",
    "            \n",
    "            #Clinical Metrics: Automatic\n",
    "            immune_counts[idx,0] = count_immune_cells(prediction, mask_AAR);\n",
    "            neuroma_counts[idx,0] =  count_neuromas(prediction, mask_AAR); \n",
    "            junction_counts[idx,0] =  count_junctions(prediction, mask_AAR); \n",
    "            nerve_densities[idx,0] =  calculate_nerve_density(prediction, mask_AAR);\n",
    "            nerve_thicknesses[idx,0] =  calculate_nerve_thickness(prediction);\n",
    "            nerve_tortuosities[idx,0] =  calculate_nerve_tortuosity(prediction); \n",
    " \n",
    "            #Clinical Metrics: Manual\n",
    "            immune_counts[idx,1] = count_immune_cells(mask, mask_AAR);\n",
    "            neuroma_counts[idx,1] = count_neuromas(mask, mask_AAR); \n",
    "            junction_counts[idx,1] = count_junctions(mask, mask_AAR); \n",
    "            nerve_densities[idx,1] =  calculate_nerve_density(mask, mask_AAR);\n",
    "            nerve_thicknesses[idx,1] = calculate_nerve_thickness(mask); \n",
    "            nerve_tortuosities[idx,1] = calculate_nerve_tortuosity(mask);  \n",
    "\n",
    "            \n",
    "            #Plot\n",
    "            if(Plot_Figures):\n",
    "\n",
    "                #Plot Original Image\n",
    "                plt.figure;\n",
    "                plt.title('Image')\n",
    "                plt.imshow(img,cmap='gray')\n",
    "                plt.show()        \n",
    "\n",
    "            if(Plot_Figures or Save_Figures):\n",
    "                \n",
    "                #Convert Mask to Color\n",
    "                color_new_mask_1 = np.zeros((384,384));\n",
    "                color_new_mask_2 = np.zeros((384,384));\n",
    "                color_new_mask_3 = np.zeros((384,384));\n",
    "                color_new_mask_1[np.where(mask==1)] = 255;\n",
    "                color_new_mask_2[np.where(mask==3)] = 255;\n",
    "                color_new_mask_3[np.where(mask==2)] = 255;\n",
    "                color_new_mask = np.zeros((384,384,3));\n",
    "                color_new_mask[:,:,0] = color_new_mask_1;\n",
    "                color_new_mask[:,:,1] = color_new_mask_3\n",
    "                color_new_mask[:,:,2] = color_new_mask_2;\n",
    "                color_new_mask = np.array(color_new_mask, dtype = 'uint8')\n",
    "            \n",
    "            #Plot\n",
    "            if(Plot_Figures):\n",
    "           \n",
    "                #Plot Mask\n",
    "                plt.figure;\n",
    "                plt.title('Ground Truth')\n",
    "                plt.imshow(color_new_mask)\n",
    "                plt.show()  \n",
    "\n",
    "            #Save Ground Truth Mask\n",
    "            if(Save_Figures):\n",
    "                    fname = 'Manual_' + str(idx) + '.jpg';               \n",
    "                    save_img = Image.fromarray(color_new_mask);\n",
    "                    save_img.save(img_path + fname)\n",
    "\n",
    "            if(Plot_Figures or Save_Figures):\n",
    "                \n",
    "                #Convert Mask to Color\n",
    "                color_new_mask_1 = np.zeros((384,384));\n",
    "                color_new_mask_2 = np.zeros((384,384));\n",
    "                color_new_mask_3 = np.zeros((384,384));\n",
    "                color_new_mask_1[np.where(prediction==1)] = 255;\n",
    "                color_new_mask_2[np.where(prediction==3)] = 255;\n",
    "                color_new_mask_3[np.where(prediction==2)] = 255;\n",
    "                color_new_mask = np.zeros((384,384,3));\n",
    "                color_new_mask[:,:,0] = color_new_mask_1;\n",
    "                color_new_mask[:,:,1] = color_new_mask_3\n",
    "                color_new_mask[:,:,2] = color_new_mask_2;\n",
    "                color_new_mask = np.array(color_new_mask, dtype = 'uint8')\n",
    "\n",
    "             #Plot\n",
    "            if(Plot_Figures):  \n",
    "                \n",
    "                #Plot Mask\n",
    "                plt.figure;\n",
    "                plt.title('Automatic')\n",
    "                plt.imshow(color_new_mask)\n",
    "                plt.show()\n",
    "\n",
    "            #Save Ground Truth Mask\n",
    "            if(Save_Figures):\n",
    "                fname = 'Automatic_' + str(idx) + '.jpg';\n",
    "                save_img = Image.fromarray(color_new_mask);\n",
    "                save_img.save(img_path + fname)                   \n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    return fscores, sensitivity, specificity, supports, immune_counts, neuroma_counts, junction_counts, nerve_densities, nerve_thicknesses, nerve_tortuosities;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716ec62-0f5e-4f86-9df3-8c35e153c67e",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8cc3b7-0877-4570-a7f2-535a64d1ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "Experiment_Name = 'Original';\n",
    "Model_Name = 'SNP_Net';\n",
    "tissues = 4;\n",
    "channels = 1;\n",
    "Plot_Figures = True;\n",
    "Save_Figures = True;\n",
    "\n",
    "\n",
    "#Path\n",
    "python_path = r'/hpc/group/viplab/zzz3/SNP_Segmentation/Files/Python/SNP-Net/';\n",
    "torch_path = r'/hpc/group/viplab/zzz3/SNP_Segmentation/Files/Torch/SNP-Net/';\n",
    "img_path = r'/hpc/group/viplab/zzz3/SNP_Segmentation/Files/Images/SNP-Net/';\n",
    "\n",
    "\n",
    "#Test AAR-Net\n",
    "fscores, sensitivity, specificity, supports, immune_counts, neuroma_counts, junction_counts, nerve_densities, nerve_thicknesses, nerve_tortuosities = test_net(python_path, torch_path, img_path, Experiment_Name, Model_Name, tissues, channels, Plot_Figures, Save_Figures);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf11119-939e-4434-aadf-60110d4b8239",
   "metadata": {},
   "source": [
    "Table 1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccee6e6-f0af-4860-9fa9-18492d19faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Zero Out\n",
    "fscores[np.where(supports==0)] = 0;\n",
    "sensitivity[np.where(supports==0)] = 0;\n",
    "\n",
    "\n",
    "#DSC\n",
    "print('=====================================================')\n",
    "print('DSC:')\n",
    "avg, stDev = weighted_avg_and_std(fscores[:,0] ,supports[:,0]);\n",
    "print('Background: ' + str(round(avg, 3)) + '; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores[:,1] ,supports[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3)) + '; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores[:,3] ,supports[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores[:,2] ,supports[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3)) + '; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Sensitivity\n",
    "print('Sensitivity:')\n",
    "avg, stDev = weighted_avg_and_std(sensitivity[:,0] ,supports[:,0]);\n",
    "print('Background: ' + str(round(avg, 3)) + '; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity[:,1] ,supports[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity[:,3] ,supports[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity[:,2] ,supports[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Specificity\n",
    "print('Specificity:')\n",
    "weight = supports[:,1] + supports[:,2] + supports[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity[:,0] ,weight);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports[:,0] + supports[:,2] + supports[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity[:,1] ,weight);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports[:,0] + supports[:,1] + supports[:,2];\n",
    "avg, stDev = weighted_avg_and_std(specificity[:,3] ,weight);\n",
    "print('Neuroma: ' + str(round(avg, 4)) +'; ' + str(round(stDev, 4)))\n",
    "weight = supports[:,0] + supports[:,1] + supports[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity[:,2] ,weight);\n",
    "print('Immune: ' + str(round(avg, 4)) +'; ' + str(round(stDev, 4)))\n",
    "print('=====================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214a964-3e11-494b-b98b-17649084c77a",
   "metadata": {},
   "source": [
    "Table 2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce56883-41c0-498d-8543-cd2c19b0373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nerve Density\n",
    "print('=====================================================')\n",
    "print('Nerve Density:')\n",
    "avg = np.nanmean(nerve_densities[:,0]); stDev = np.nanstd(nerve_densities[:,0]);\n",
    "print('Automatic: ' + str(round(avg))+ '; ' + str(round(stDev)))\n",
    "avg = np.mean(nerve_densities[:,1]); stDev = np.std(nerve_densities[:,1]);\n",
    "print('Manual: ' + str(round(avg)) +'; ' + str(round(stDev)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Average Nerve Thickness\n",
    "print('Average Nerve Thickness:')\n",
    "avg = np.nanmean(nerve_thicknesses[:,0]); stDev = np.nanstd(nerve_thicknesses[:,0]);\n",
    "print('Automatic: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "avg = np.nanmean(nerve_thicknesses[:,1]); stDev = np.nanstd(nerve_thicknesses[:,1]);\n",
    "print('Manual: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Average Nerve Segment Tortuosity\n",
    "print('Average Nerve Segment Tortuosity:')\n",
    "avg = np.nanmean(nerve_tortuosities[:,0]); stDev = np.nanstd(nerve_tortuosities[:,0]);\n",
    "print('Automatic: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "avg = np.nanmean(nerve_tortuosities[:,1]); stDev = np.nanstd(nerve_tortuosities[:,1]);\n",
    "print('Manual: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Junction Point Density\n",
    "print('Junction Point Density:')\n",
    "avg = np.nanmean(junction_counts[:,0]); stDev = np.nanstd(junction_counts[:,0]);\n",
    "print('Automatic: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "avg = np.nanmean(junction_counts[:,1]); stDev = np.nanstd(junction_counts[:,1]);\n",
    "print('Manual: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "print('=====================================================')\n",
    "             \n",
    "#Neuroma Density\n",
    "print('Neuroma Density:')\n",
    "avg = np.nanmean(neuroma_counts[:,0]); stDev = np.nanstd(neuroma_counts[:,0]);\n",
    "print('Automatic: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "avg = np.nanmean(neuroma_counts[:,1]); stDev = np.nanstd(neuroma_counts[:,1]);\n",
    "print('Manual: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))             \n",
    "print('=====================================================') \n",
    "             \n",
    "#Immune Cell Density\n",
    "print('Immune Cell Density:')\n",
    "avg = np.nanmean(immune_counts[:,0]); stDev = np.nanstd(immune_counts[:,0]);\n",
    "print('Automatic: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1)))\n",
    "avg = np.nanmean(immune_counts[:,1]); stDev = np.nanstd(immune_counts[:,1]);\n",
    "print('Manual: ' + str(round(avg, 1)) +'; ' + str(round(stDev, 1))) \n",
    "print('=====================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47a283-5288-43fb-a995-615b31aaf432",
   "metadata": {},
   "source": [
    "Intraclass Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc001c-bb63-4355-8613-b56b37ae8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Intraclass Correlation\n",
    "a = np.arange(0,207);\n",
    "aa = np.concatenate((a,a), axis = 0);\n",
    "b0 = np.zeros((207,1)).squeeze();\n",
    "b1 = np.ones((207,1)).squeeze();\n",
    "bb = np.concatenate((b0,b1), axis = 0);\n",
    "\n",
    "cc = np.concatenate((nerve_densities[:,0], nerve_densities[:,1]),axis=0)\n",
    "#cc = np.concatenate((nerve_thicknesses[:,0], nerve_thicknesses[:,1]),axis=0)\n",
    "#cc = np.concatenate((nerve_tortuosities[:,0], nerve_tortuosities[:,1]),axis=0)\n",
    "#cc = np.concatenate((junction_counts[:,0], junction_counts[:,1]),axis=0)\n",
    "#cc = np.concatenate((neuroma_counts[:,0], neuroma_counts[:,1]),axis=0) \n",
    "#cc = np.concatenate((immune_counts[:,0], immune_counts[:,1]),axis=0)  \n",
    "\n",
    "df = pd.DataFrame({'exam': aa, 'judge': bb, 'rating': cc})\n",
    "icc = pg.intraclass_corr(data=df, targets='exam', raters='judge', ratings='rating')\n",
    "icc.set_index('Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b86921-d982-49e7-831f-afcaf1eb7a4e",
   "metadata": {},
   "source": [
    "Table 3 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f26acd-b297-4a1d-95f5-a0c98144af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Numpy\n",
    "other_grader = np.load(python_path + 'SNP_Net_Other_Grader.npy'); \n",
    "masks_other = np.load(python_path + 'SNP_Net_Masks_Other.npy');\n",
    "masks = np.load(python_path + 'SNP_Net_Masks.npy');\n",
    "masks_original = masks[other_grader];\n",
    "\n",
    "#Automatic versus Reader 1\n",
    "fscores_original = fscores[other_grader];\n",
    "sensitivity_original = sensitivity[other_grader];\n",
    "specificity_original = specificity[other_grader];\n",
    "supports_original = supports[other_grader];\n",
    "\n",
    "\n",
    "\n",
    "print('=====================================================')\n",
    "print('SNP-Net versus Reader 1')\n",
    "print('=====================================================')\n",
    "\n",
    "#DSC\n",
    "print('DSC:')\n",
    "avg, stDev = weighted_avg_and_std(fscores_original[:,0] ,supports_original[:,0]);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_original[:,1] ,supports_original[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_original[:,3] ,supports_original[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_original[:,2] ,supports_original[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Sensitivity\n",
    "print('Sensitivity:')\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_original[:,0] ,supports_original[:,0]);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_original[:,1] ,supports_original[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_original[:,3] ,supports_original[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_original[:,2] ,supports_original[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Specificity\n",
    "print('Specificity:')\n",
    "weight = supports_original[:,1] + supports_original[:,2] + supports_original[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_original[:,0] ,weight);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports_original[:,0] + supports_original[:,2] + supports_original[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_original[:,1] ,weight);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports_original[:,0] + supports_original[:,1] + supports_original[:,2];\n",
    "avg, stDev = weighted_avg_and_std(specificity_original[:,3] ,weight);\n",
    "print('Neuroma: ' + str(round(avg, 4)) +'; ' + str(round(stDev, 4)))\n",
    "weight = supports_original[:,0] + supports_original[:,1] + supports_original[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_original[:,2] ,weight);\n",
    "print('Immune: ' + str(round(avg, 4))+ '; ' + str(round(stDev, 4)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Initialize\n",
    "fscores_other = np.zeros((np.shape(masks_other)[0], tissues));\n",
    "sensitivity_other = np.zeros((np.shape(masks_other)[0], tissues));\n",
    "specificity_other = np.zeros((np.shape(masks_other)[0], tissues));\n",
    "supports_other = np.zeros((np.shape(masks_other)[0], tissues));\n",
    "\n",
    "#Iterate\n",
    "for idx in range(0, np.shape(masks_other)[0]):\n",
    "    \n",
    "    #Mask\n",
    "    mask_original = masks_original[idx,:,:].squeeze();\n",
    "    mask_other = masks_other[idx,:,:].squeeze();\n",
    "    \n",
    "    \n",
    "    #Calculate Metrics\n",
    "    cm = ConfusionMatrix(tissues)\n",
    "    cm.add(mask_other, mask_original)\n",
    "    acc_out = segmentation_metrics(cm);\n",
    "    sens_per_class = cm.get_sensitivity_per_class();\n",
    "    spec_per_class = cm.get_specificity_per_class();\n",
    "    sensitivity_other[idx,0:4] = sens_per_class;\n",
    "    specificity_other[idx,0:4] = spec_per_class;       \n",
    "    temp_1 =   mask_original.flatten();\n",
    "    temp_2 =   mask_other.flatten();\n",
    "    x = np.array([0,1,2,3]).astype('float64');\n",
    "    temp_1 =  np.concatenate((temp_1, x))\n",
    "    temp_2 =  np.concatenate((temp_2, x))\n",
    "    precision,recall,fscore,support=score(temp_1,temp_2)\n",
    "    fscores_other[idx,0:4] = fscore;\n",
    "    supports_other[idx,0:4] = support -1; \n",
    "    \n",
    "\n",
    "#Zero Out\n",
    "fscores_other[np.where(supports_other==0)] = 0;\n",
    "sensitivity_other[np.where(supports_other==0)] = 0;\n",
    "\n",
    "    \n",
    "print('=====================================================')\n",
    "print('Reader 2 & 3 versus Reader 1')\n",
    "print('=====================================================')\n",
    "    \n",
    "#DSC\n",
    "print('DSC:')\n",
    "avg, stDev = weighted_avg_and_std(fscores_other[:,0] ,supports_other[:,0]);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_other[:,1] ,supports_other[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_other[:,3] ,supports_other[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(fscores_other[:,2] ,supports_other[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3))+ '; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "#Sensitivity\n",
    "print('Sensitivity:')\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_other[:,0] ,supports_other[:,0]);\n",
    "print('Background: ' + str(round(avg, 3))+ '; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_other[:,1] ,supports_other[:,1]);\n",
    "print('Nerve: ' + str(round(avg, 3))+ '; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_other[:,3] ,supports_other[:,3]);\n",
    "print('Neuroma: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "avg, stDev = weighted_avg_and_std(sensitivity_other[:,2] ,supports_other[:,2]);\n",
    "print('Immune: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "print('=====================================================')\n",
    "\n",
    "\n",
    "#Specificity\n",
    "print('Specificity:')\n",
    "weight = supports_other[:,1] + supports_other[:,2] + supports_other[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_other[:,0] ,weight);\n",
    "print('Background: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports_other[:,0] + supports_other[:,2] + supports_other[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_other[:,1] ,weight);\n",
    "print('Nerve: ' + str(round(avg, 3)) +'; ' + str(round(stDev, 3)))\n",
    "weight = supports_other[:,0] + supports_other[:,1] + supports_other[:,2];\n",
    "avg, stDev = weighted_avg_and_std(specificity_other[:,3] ,weight);\n",
    "print('Neuroma: ' + str(round(avg, 4)) +'; ' + str(round(stDev, 4)))\n",
    "weight = supports_other[:,0] + supports_other[:,1] + supports_other[:,3];\n",
    "avg, stDev = weighted_avg_and_std(specificity_other[:,2] ,weight);\n",
    "print('Immune: ' + str(round(avg, 4)) + '; ' + str(round(stDev, 4)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#Calculate Difference\n",
    "fscores_diff = fscores_original - fscores_other;\n",
    "sensitivity_diff = sensitivity_original - sensitivity_other;\n",
    "specificity_diff = specificity_original - specificity_other;\n",
    "\n",
    "\n",
    "#Ignore Certain Values\n",
    "fscores_diff[np.where(supports_other==0.0)] = 'NaN';\n",
    "sensitivity_diff[np.where(supports_other==0.0)] = 'NaN';\n",
    "\n",
    "\n",
    "\n",
    "print('=====================================================')\n",
    "print('P-Values: SNP-Net Greater')\n",
    "print('=====================================================')\n",
    "\n",
    "\n",
    "#DSC\n",
    "print('DSC:')\n",
    "res = wilcoxon(fscores_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(fscores_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(fscores_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(fscores_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')\n",
    "\n",
    "#Sensitivity\n",
    "print('Sensitivity:')\n",
    "res = wilcoxon(sensitivity_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(sensitivity_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(sensitivity_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(sensitivity_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')\n",
    "\n",
    "#Specificity\n",
    "print('Specificity:')\n",
    "res = wilcoxon(specificity_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(specificity_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(specificity_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(specificity_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')\n",
    "\n",
    "\n",
    "fscores_diff = fscores_other - fscores_original;\n",
    "sensitivity_diff = sensitivity_other - sensitivity_original;\n",
    "specificity_diff = specificity_other - specificity_original;\n",
    "\n",
    "\n",
    "\n",
    "#Ignore Certain Values\n",
    "fscores_diff[np.where(supports_other==0.0)] = 'NaN';\n",
    "sensitivity_diff[np.where(supports_other==0.0)] = 'NaN';\n",
    "\n",
    "\n",
    "print('=====================================================')\n",
    "print('P-Values: Other Graders Greater')\n",
    "print('=====================================================')\n",
    "\n",
    "\n",
    "#DSC\n",
    "print('DSC:')\n",
    "res = wilcoxon(fscores_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(fscores_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(fscores_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(fscores_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')\n",
    "\n",
    "#Sensitivity\n",
    "print('Sensitivity:')\n",
    "res = wilcoxon(sensitivity_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(sensitivity_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(sensitivity_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(sensitivity_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')\n",
    "\n",
    "#Specificity\n",
    "print('Specificity:')\n",
    "res = wilcoxon(specificity_diff[:,0], alternative='greater', nan_policy='omit')\n",
    "print('Background: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(specificity_diff[:,1], alternative='greater', nan_policy='omit')\n",
    "print('Nerve: ' + str(round(res.pvalue, 3)))\n",
    "res = wilcoxon(specificity_diff[:,3], alternative='greater', nan_policy='omit')\n",
    "\n",
    "print('Neuroma: ' + str(round(res.pvalue, 3)) )\n",
    "res = wilcoxon(specificity_diff[:,2], alternative='greater', nan_policy='omit')\n",
    "print('Immune: ' + str(round(res.pvalue, 3)) )\n",
    "print('=====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40f9f1-067f-407b-b4ec-a48419fed396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
